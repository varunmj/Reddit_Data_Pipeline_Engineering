[2024-08-08T21:16:19.283+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-08T21:16:19.316+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-08T21:16:17.793694+00:00 [queued]>
[2024-08-08T21:16:19.323+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-08T21:16:17.793694+00:00 [queued]>
[2024-08-08T21:16:19.323+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-08-08T21:16:19.338+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-08-08 21:16:17.793694+00:00
[2024-08-08T21:16:19.355+0000] {standard_task_runner.py:64} INFO - Started process 52 to run task
[2024-08-08T21:16:19.365+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-08-08T21:16:17.793694+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmple33ec6x']
[2024-08-08T21:16:19.378+0000] {standard_task_runner.py:91} INFO - Job 18: Subtask reddit_extraction
[2024-08-08T21:16:19.579+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-08T21:16:17.793694+00:00 [running]> on host 330977813b6b
[2024-08-08T21:16:19.820+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Varun MJ' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-08-08T21:16:17.793694+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-08-08T21:16:17.793694+00:00'
[2024-08-08T21:16:19.821+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-08T21:16:19.852+0000] {logging_mixin.py:188} INFO - Connected to reddit!
[2024-08-08T21:16:20.637+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_k0kr23gm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Job Description vs. The Job', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en6uiw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'ups': 185, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'reddit_video': {'bitrate_kbps': 1200, 'fallback_url': 'https://v.redd.it/cofdka9z9ehd1/DASH_480.mp4?source=fallback', 'has_audio': True, 'height': 480, 'width': 722, 'scrubber_media_url': 'https://v.redd.it/cofdka9z9ehd1/DASH_96.mp4', 'dash_url': 'https://v.redd.it/cofdka9z9ehd1/DASHPlaylist.mpd?a=1725743780%2COTM4ZGZhZWVjNzRlNWNmMjExZTFiODdkOTE5NWJkYzc0NmQ5ZDU1ZjAzOGMwYzM1YmQ3MGI0ZTc5M2QxYWQxNA%3D%3D&v=1&f=sd', 'duration': 31, 'hls_url': 'https://v.redd.it/cofdka9z9ehd1/HLSPlaylist.m3u8?a=1725743780%2CNDY5ZWE5YmMxZDBjNjM5N2E4NzYyMTNjMGFlNGU5MTgxYzUwMmM4N2U2NTVhNGNlNmIzYzJlMjFmMGVlZmEwNw%3D%3D&v=1&f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 185, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?width=140&height=93&crop=140:93,smart&format=jpg&v=enabled&lthumb=true&s=be33d8d4f6ad9cd0b4e60daf1b0df9a218d7845c', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'hosted:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1723125894.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'v.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://v.redd.it/cofdka9z9ehd1', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?format=pjpg&auto=webp&s=d88c276ab72158c676b86beeb823e8c2dbeeae21', 'width': 800, 'height': 532}, 'resolutions': [{'url': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?width=108&crop=smart&format=pjpg&auto=webp&s=5b7cf5fd74ac0e1de743ba6a377a03b3ac88249c', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?width=216&crop=smart&format=pjpg&auto=webp&s=be8c59a5487ebe7bae33471d8c95ed1817d44d05', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?width=320&crop=smart&format=pjpg&auto=webp&s=00949a0d564fb5126a9ebec337e573ff639c119d', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX.png?width=640&crop=smart&format=pjpg&auto=webp&s=74967d004dbcb7f5ab7751ddfb62272fd8121022', 'width': 640, 'height': 425}], 'variants': {}, 'id': 'dG5wbGRiYXo5ZWhkMTih-IorhvjlbcRmB3wey7pjG2SrrcqaBDG7QwIApfMX'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1en6uiw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SelectStarData'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en6uiw/the_job_description_vs_the_job/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://v.redd.it/cofdka9z9ehd1', 'subreddit_subscribers': 203035, 'created_utc': 1723125894.0, 'num_crossposts': 0, 'media': {'reddit_video': {'bitrate_kbps': 1200, 'fallback_url': 'https://v.redd.it/cofdka9z9ehd1/DASH_480.mp4?source=fallback', 'has_audio': True, 'height': 480, 'width': 722, 'scrubber_media_url': 'https://v.redd.it/cofdka9z9ehd1/DASH_96.mp4', 'dash_url': 'https://v.redd.it/cofdka9z9ehd1/DASHPlaylist.mpd?a=1725743780%2COTM4ZGZhZWVjNzRlNWNmMjExZTFiODdkOTE5NWJkYzc0NmQ5ZDU1ZjAzOGMwYzM1YmQ3MGI0ZTc5M2QxYWQxNA%3D%3D&v=1&f=sd', 'duration': 31, 'hls_url': 'https://v.redd.it/cofdka9z9ehd1/HLSPlaylist.m3u8?a=1725743780%2CNDY5ZWE5YmMxZDBjNjM5N2E4NzYyMTNjMGFlNGU5MTgxYzUwMmM4N2U2NTVhNGNlNmIzYzJlMjFmMGVlZmEwNw%3D%3D&v=1&f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_video': True, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.638+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm in the process for a Tech Support Data Engineer role.  \n  \n1. Video Screening with HR Recruiter. - Done  \n2. TestGorilla Data Engineering Assessment: I will take a comprehensive assessment on TestGorilla that includes 10 different tests over a 2-hour period, where I must score over 75%.  \n3. Data Engineer Take-Home Assessment.  \n4. Panel Meeting: This involves two segments:  \n- SQL Technical Assessment: The first 30 minutes will be dedicated to evaluating my SQL skills through a technical assessment.  \n- Experience Discussion: The next 30 minutes will focus on discussing my past experiences, projects, and how they relate to the Data Engineer position.  \n5. SQL Assessment and Customer-Facing Problem: I will participate in an additional SQL assessment and solve a problem involving customer interactions with an overseas client.  \n6. Meeting with Hiring Manager: I will have a one-on-one meeting with the Hiring Manager, which will include behavioural questions.  \n7. Onsite Evaluation.  \n8. Final meeting with Overseas Client.\n\nEDIT: Pay is $53k. One of the benefits is that they also have a team of mental health specialists, similar to those in the Billions TV series. I will also be on probation for a month and will have two weeks of training.", 'author_fullname': 't2_i4gjp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How many rounds are too many for a Data Engineer role?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en3oq2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 55, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723138087.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723117132.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m in the process for a Tech Support Data Engineer role.  </p>\n\n<ol>\n<li>Video Screening with HR Recruiter. - Done<br/></li>\n<li>TestGorilla Data Engineering Assessment: I will take a comprehensive assessment on TestGorilla that includes 10 different tests over a 2-hour period, where I must score over 75%.<br/></li>\n<li>Data Engineer Take-Home Assessment.<br/></li>\n<li>Panel Meeting: This involves two segments:<br/></li>\n<li>SQL Technical Assessment: The first 30 minutes will be dedicated to evaluating my SQL skills through a technical assessment.<br/></li>\n<li>Experience Discussion: The next 30 minutes will focus on discussing my past experiences, projects, and how they relate to the Data Engineer position.<br/></li>\n<li>SQL Assessment and Customer-Facing Problem: I will participate in an additional SQL assessment and solve a problem involving customer interactions with an overseas client.<br/></li>\n<li>Meeting with Hiring Manager: I will have a one-on-one meeting with the Hiring Manager, which will include behavioural questions.<br/></li>\n<li>Onsite Evaluation.<br/></li>\n<li>Final meeting with Overseas Client.</li>\n</ol>\n\n<p>EDIT: Pay is $53k. One of the benefits is that they also have a team of mental health specialists, similar to those in the Billions TV series. I will also be on probation for a month and will have two weeks of training.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1en3oq2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Greckol'), 'discussion_type': None, 'num_comments': 101, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en3oq2/how_many_rounds_are_too_many_for_a_data_engineer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en3oq2/how_many_rounds_are_too_many_for_a_data_engineer/', 'subreddit_subscribers': 203035, 'created_utc': 1723117132.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.638+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I just started as a DE at an older company and they use a bunch of outdated systems besides a few pieces of software and I feel incredibly lost and confused. Everything I’ve learned in school used more modern approaches like dbt, mage or dagster. They use a little bit of airflow but mostly low / no-code for ETL. I’m a new grad so I took the job for the experience but any advice or words of advice I’d appreciate.', 'author_fullname': 't2_cnnznqhp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DE job struggle', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emst8g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 42, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 42, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723078950.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I just started as a DE at an older company and they use a bunch of outdated systems besides a few pieces of software and I feel incredibly lost and confused. Everything I’ve learned in school used more modern approaches like dbt, mage or dagster. They use a little bit of airflow but mostly low / no-code for ETL. I’m a new grad so I took the job for the experience but any advice or words of advice I’d appreciate.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1emst8g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pl0nt_lvr'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emst8g/de_job_struggle/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emst8g/de_job_struggle/', 'subreddit_subscribers': 203035, 'created_utc': 1723078950.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.639+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Honest question here, I've seen time and time again a pattern where a centralized data store is added to ETL raw data and effectively prevent downstream users from accessing raw data.\n\nInvariably, mega tables with dozens or hundreds of columns are created and become a dependency of basically everything in the company.  Those tables are then either\n\n(1) Constantly in flux as everyone needs to touch it to fix things, causing errors across the company\n\n(2) Way too scary to touch, so bad patterns are never fixed\n\nNaively? I wonder if this could be avoided if instead each downstream team owns their own ETL logic over the raw outputs (read only copies / pubsub) of applications.\n\nWhy is it so common to avoid sharing the more raw data?\n", 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why do teams avoid sharing "raw" data?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enacru', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': '', 'subreddit_type': 'public', 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723134317.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Honest question here, I&#39;ve seen time and time again a pattern where a centralized data store is added to ETL raw data and effectively prevent downstream users from accessing raw data.</p>\n\n<p>Invariably, mega tables with dozens or hundreds of columns are created and become a dependency of basically everything in the company.  Those tables are then either</p>\n\n<p>(1) Constantly in flux as everyone needs to touch it to fix things, causing errors across the company</p>\n\n<p>(2) Way too scary to touch, so bad patterns are never fixed</p>\n\n<p>Naively? I wonder if this could be avoided if instead each downstream team owns their own ETL logic over the raw outputs (read only copies / pubsub) of applications.</p>\n\n<p>Why is it so common to avoid sharing the more raw data?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1enacru', 'is_robot_indexable': True, 'report_reasons': None, 'author': None, 'discussion_type': None, 'num_comments': 45, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1enacru/why_do_teams_avoid_sharing_raw_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enacru/why_do_teams_avoid_sharing_raw_data/', 'subreddit_subscribers': 203035, 'created_utc': 1723134317.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.639+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am a data engineer with 6 years of experience. Quiet proficient in langs like python,sql.\n\nWhen it comes to cloud platforms i have worked majorly in aws( athena glue emr lambda ec2 redshift s3 etc) and also on databricks ( aws as infra) and snowflake.\n\nDuring the last year or so or even more, i see companies hiring more for azure data engineers ! \n\nFrom my understanding azure offers smooth integration with the microsoft world along with cheaper licensing maybe so companies use azure.\n\nFrom what i see :-\nBig company( allowing more freedom to developers) and midsize / startups- prefer more aws \n\nBanks/ oil companies/ and other big players (IT companies) - would go for azure since majority of their work is on windows ( maybe? )\n\n8/10 jobs i apply to have the word (azure) before ‘data engineer’ and i do convey the fact that since basics of data engineering are the same i would be able to switch to the new cloud in no time but ultimately fail to get a job.\n\nI am not saying azure is the market leader but it looks like its the transition to cloud leader for quite sometime.\n\nI was quite confident of my aws skills but I recently i feel i should learn and build projects on azure so atleast i have something to convey or showcase but then i also think it wont make any difference as most of my official work is on aws and i am a mid level engineer.\n\nI in no way mean to compare the 2 cloud platforms, for me its merely a cloud provider but from what i see azure devops/ azure data engineer might be more in demand so that makes me want to think whether i should start upskilling on azure.\n\nCountry of residence- Netherlands.', 'author_fullname': 't2_6xv3zxp7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineer (Aws) but azure ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en2q3i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723113800.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am a data engineer with 6 years of experience. Quiet proficient in langs like python,sql.</p>\n\n<p>When it comes to cloud platforms i have worked majorly in aws( athena glue emr lambda ec2 redshift s3 etc) and also on databricks ( aws as infra) and snowflake.</p>\n\n<p>During the last year or so or even more, i see companies hiring more for azure data engineers ! </p>\n\n<p>From my understanding azure offers smooth integration with the microsoft world along with cheaper licensing maybe so companies use azure.</p>\n\n<p>From what i see :-\nBig company( allowing more freedom to developers) and midsize / startups- prefer more aws </p>\n\n<p>Banks/ oil companies/ and other big players (IT companies) - would go for azure since majority of their work is on windows ( maybe? )</p>\n\n<p>8/10 jobs i apply to have the word (azure) before ‘data engineer’ and i do convey the fact that since basics of data engineering are the same i would be able to switch to the new cloud in no time but ultimately fail to get a job.</p>\n\n<p>I am not saying azure is the market leader but it looks like its the transition to cloud leader for quite sometime.</p>\n\n<p>I was quite confident of my aws skills but I recently i feel i should learn and build projects on azure so atleast i have something to convey or showcase but then i also think it wont make any difference as most of my official work is on aws and i am a mid level engineer.</p>\n\n<p>I in no way mean to compare the 2 cloud platforms, for me its merely a cloud provider but from what i see azure devops/ azure data engineer might be more in demand so that makes me want to think whether i should start upskilling on azure.</p>\n\n<p>Country of residence- Netherlands.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1en2q3i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Technical-Tap-5424'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en2q3i/data_engineer_aws_but_azure/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en2q3i/data_engineer_aws_but_azure/', 'subreddit_subscribers': 203035, 'created_utc': 1723113800.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.639+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_g9pekhtm4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Computing Option Greeks in real time using Pathway and Databento', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en3oty', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'default', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1723117142.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'pathway.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://pathway.com/developers/templates/option-greeks', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1en3oty', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LocksmithBest2231'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en3oty/computing_option_greeks_in_real_time_using/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://pathway.com/developers/templates/option-greeks', 'subreddit_subscribers': 203035, 'created_utc': 1723117142.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.640+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey all,\n\nLurker and fairly new DE, absolutely loving it so far.\n\nI have a challenge which seems trivial yet don't understand what is going on here performance wise despite trying multiple things.\n\nTech wise DBX 14.3\n\nI have a streaming delta, let's say this is a table with 8 columns and one column is a linking key string (24 char). The batch size of this would maybe be 30 records for argument sake. \n\nI need to join this streaming table to a static data frame of ~1bn rows on this key. This dataframe has around 35 columns.\n\nThe keyings are in the first 32 fields.\n\nCurrently when trying to do left joins, when the source df is 80 rows or even 1 row the join never completes (have let it run hours with 3TB of write shuffle).\n\nThis has confused me because ultimately at 1bn rows it seems a small problem.\n\nI have tried z indexing on the key field, this seemingly has no change. I have also tried a liquid clustering POC using deep clone and have same performance there. Both these scenarios I ran optimize and auto compact.\n\nIt's almost like the execution plan doesn't care about these things existing.\n\nThe only leading theory I have right now, is that no one has ever ran analyze table compute stats in our estate ever. All the documentation suggests you need to run this, at least after doing z ordering for example.\n\nAny help would be much appreciated! ", 'author_fullname': 't2_2gd12jc3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Super unperformant join delta to delta', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emzs08', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723102059.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all,</p>\n\n<p>Lurker and fairly new DE, absolutely loving it so far.</p>\n\n<p>I have a challenge which seems trivial yet don&#39;t understand what is going on here performance wise despite trying multiple things.</p>\n\n<p>Tech wise DBX 14.3</p>\n\n<p>I have a streaming delta, let&#39;s say this is a table with 8 columns and one column is a linking key string (24 char). The batch size of this would maybe be 30 records for argument sake. </p>\n\n<p>I need to join this streaming table to a static data frame of ~1bn rows on this key. This dataframe has around 35 columns.</p>\n\n<p>The keyings are in the first 32 fields.</p>\n\n<p>Currently when trying to do left joins, when the source df is 80 rows or even 1 row the join never completes (have let it run hours with 3TB of write shuffle).</p>\n\n<p>This has confused me because ultimately at 1bn rows it seems a small problem.</p>\n\n<p>I have tried z indexing on the key field, this seemingly has no change. I have also tried a liquid clustering POC using deep clone and have same performance there. Both these scenarios I ran optimize and auto compact.</p>\n\n<p>It&#39;s almost like the execution plan doesn&#39;t care about these things existing.</p>\n\n<p>The only leading theory I have right now, is that no one has ever ran analyze table compute stats in our estate ever. All the documentation suggests you need to run this, at least after doing z ordering for example.</p>\n\n<p>Any help would be much appreciated! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1emzs08', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RexehBRS'), 'discussion_type': None, 'num_comments': 20, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emzs08/super_unperformant_join_delta_to_delta/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emzs08/super_unperformant_join_delta_to_delta/', 'subreddit_subscribers': 203035, 'created_utc': 1723102059.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.640+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi Friends!\n\nI have stumbled into an interesting situation. I recently started working for an outdoor activity company as a photographer. On my first day, the person training me was showing me how to copy and paste customers information into a spreadsheet lol. I told him I was just going to automate the task for us and so I did. The owner of the company heard about this and was STOKED. They essentially hunted me down in the office one day to tell me how happy they were with me and that they are very interested in getting me working on some more \'important\' work.\n\nA couple weeks have gone by now and the owner and I had a chat. They let me know that they are very interested in keeping me on the team as we move into the off season and that they are aware that I am overqualified for my current role and "seriously underpaid" and so they told me to let them know my \'normal rate\' as i start doing some data work for them. So, I have since been given admin privs to both of the companies CRM websites as well as root access to their server which leads me to some questions. I have essentially been given the green light to do whatever I want to help the company turn their data into actionable insight but I am not even sure where to begin.\n\nThe company doesn\'t store any of their own information in a database, it\'s all stored on the CRM websites they use. So, my first thought was to set up their own database with their customer, trip, sales etc. data. But is this even necessary? I can access pretty much anything I would want to know about their customers through the CRM APIs. I guess I am just not used to having so much autonomy at a company and don\'t know where to begin on this data project that has absolutely no direction. I was wondering if y\'all had any advice on projects I could start working on to demonstrate the potential value having a DE on their team could bring. I think another issue I am having is that the owner likes to rely on their \'gut instincts\' and knows absolutely nothing about computers so it\'s hard to even have a meaningful conversation about data. \n\nUltimately, I am just looking for any advice or ideas yall have that I can start working on or pitch to the owner to help the company but also ensure some job security for myself. For what it\'s worth, I have about 3 years of DE experience where I mostly did ETL work. \n\nTIA!', 'author_fullname': 't2_6pt3in3k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for advice..', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emvjvg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723086917.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Friends!</p>\n\n<p>I have stumbled into an interesting situation. I recently started working for an outdoor activity company as a photographer. On my first day, the person training me was showing me how to copy and paste customers information into a spreadsheet lol. I told him I was just going to automate the task for us and so I did. The owner of the company heard about this and was STOKED. They essentially hunted me down in the office one day to tell me how happy they were with me and that they are very interested in getting me working on some more &#39;important&#39; work.</p>\n\n<p>A couple weeks have gone by now and the owner and I had a chat. They let me know that they are very interested in keeping me on the team as we move into the off season and that they are aware that I am overqualified for my current role and &quot;seriously underpaid&quot; and so they told me to let them know my &#39;normal rate&#39; as i start doing some data work for them. So, I have since been given admin privs to both of the companies CRM websites as well as root access to their server which leads me to some questions. I have essentially been given the green light to do whatever I want to help the company turn their data into actionable insight but I am not even sure where to begin.</p>\n\n<p>The company doesn&#39;t store any of their own information in a database, it&#39;s all stored on the CRM websites they use. So, my first thought was to set up their own database with their customer, trip, sales etc. data. But is this even necessary? I can access pretty much anything I would want to know about their customers through the CRM APIs. I guess I am just not used to having so much autonomy at a company and don&#39;t know where to begin on this data project that has absolutely no direction. I was wondering if y&#39;all had any advice on projects I could start working on to demonstrate the potential value having a DE on their team could bring. I think another issue I am having is that the owner likes to rely on their &#39;gut instincts&#39; and knows absolutely nothing about computers so it&#39;s hard to even have a meaningful conversation about data. </p>\n\n<p>Ultimately, I am just looking for any advice or ideas yall have that I can start working on or pitch to the owner to help the company but also ensure some job security for myself. For what it&#39;s worth, I have about 3 years of DE experience where I mostly did ETL work. </p>\n\n<p>TIA!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1emvjvg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lilj8812'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emvjvg/looking_for_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emvjvg/looking_for_advice/', 'subreddit_subscribers': 203035, 'created_utc': 1723086917.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.640+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'TLDR: where are the design/project patterns for lake(house)s?\n\nFor OLTP, you have Normal Forms / ER. For traditional DW, you\'ve got Star Schema. Both are very well documented and you can find lots of advice and design patterns for implementing either.\n\nFor lake(like) technologies, you get the advice to have layers (the medallion architecture), but not much else (at least that I could find). You\'d think there would be design patterns/approaches for this that are well documented, but it\'s hard to find (other than the "have layers" advice). \n\n  \nExample (just off the top of my head).\n\n* If you\'re pre-joining data in the silver or gold tiers, how do you handle cartesian products? \n* If data doesn\'t meet some quality requirement, what happens? Does it get marked in the target object somehow? Put in a separate structure?\n* What are the best practices for creating aggregates?\n* How do you handle master data that differs between sources?\n* What\'s the best business / project approach for designing/testing these? How should observability actually be implemented?\n\nIs there a book or course I just haven\'t seen?', 'author_fullname': 't2_ahf8v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Lake (Structure) Design Patterns', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en8yrn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723130995.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>TLDR: where are the design/project patterns for lake(house)s?</p>\n\n<p>For OLTP, you have Normal Forms / ER. For traditional DW, you&#39;ve got Star Schema. Both are very well documented and you can find lots of advice and design patterns for implementing either.</p>\n\n<p>For lake(like) technologies, you get the advice to have layers (the medallion architecture), but not much else (at least that I could find). You&#39;d think there would be design patterns/approaches for this that are well documented, but it&#39;s hard to find (other than the &quot;have layers&quot; advice). </p>\n\n<p>Example (just off the top of my head).</p>\n\n<ul>\n<li>If you&#39;re pre-joining data in the silver or gold tiers, how do you handle cartesian products? </li>\n<li>If data doesn&#39;t meet some quality requirement, what happens? Does it get marked in the target object somehow? Put in a separate structure?</li>\n<li>What are the best practices for creating aggregates?</li>\n<li>How do you handle master data that differs between sources?</li>\n<li>What&#39;s the best business / project approach for designing/testing these? How should observability actually be implemented?</li>\n</ul>\n\n<p>Is there a book or course I just haven&#39;t seen?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1en8yrn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='PencilBoy99'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en8yrn/lake_structure_design_patterns/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en8yrn/lake_structure_design_patterns/', 'subreddit_subscribers': 203035, 'created_utc': 1723130995.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.640+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone, I typically haven’t had to work with different data sources supplying different date granularities. If you had sales data coming in from different sources at daily, weekly, monthly from different vendors how would you typically handle it in dimensional modelling? \n\n(1) Would you allocate the higher granularity sources down to the daily level using a first day of the week, first day of month etc? So that all data is then at the lowest available granularity even if some of the data sources won’t be accurate at that granularity?\n\n(2) Or would you aggregate the lower granularity sources up to the highest granularity and lose the detail for some data sources?\n\n(3) or would you have multiple fact tables for daily, weekly, monthly etc. in this option would you still load all vendor data sources to all granularities to ensure all data is present at each level? \n\nThis would have powerbi sitting on top of it in case that is relevant.\n\nCheers', 'author_fullname': 't2_rqv6y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Modelling different date granularities ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en0d58', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723104482.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone, I typically haven’t had to work with different data sources supplying different date granularities. If you had sales data coming in from different sources at daily, weekly, monthly from different vendors how would you typically handle it in dimensional modelling? </p>\n\n<p>(1) Would you allocate the higher granularity sources down to the daily level using a first day of the week, first day of month etc? So that all data is then at the lowest available granularity even if some of the data sources won’t be accurate at that granularity?</p>\n\n<p>(2) Or would you aggregate the lower granularity sources up to the highest granularity and lose the detail for some data sources?</p>\n\n<p>(3) or would you have multiple fact tables for daily, weekly, monthly etc. in this option would you still load all vendor data sources to all granularities to ensure all data is present at each level? </p>\n\n<p>This would have powerbi sitting on top of it in case that is relevant.</p>\n\n<p>Cheers</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1en0d58', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DarthBallz999'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en0d58/modelling_different_date_granularities/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en0d58/modelling_different_date_granularities/', 'subreddit_subscribers': 203035, 'created_utc': 1723104482.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'What tips would you give someone who starts out from scratch?\n\nLike we have clients who send their excel spreadsheets to us and our domain experts do stuff with ths data, and now I was hired as a data engineer to improve their process', 'author_fullname': 't2_ay94x4g96', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do companies that deal with a large amount of excel spreatsheet data from various clients that have different standards for their data? Do they keep them as spreadsheets? Do they convert them into SQL databases or NoSQL databases?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enbzb3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723138194.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>What tips would you give someone who starts out from scratch?</p>\n\n<p>Like we have clients who send their excel spreadsheets to us and our domain experts do stuff with ths data, and now I was hired as a data engineer to improve their process</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1enbzb3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='WishIWasBronze'), 'discussion_type': None, 'num_comments': 18, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enbzb3/how_do_companies_that_deal_with_a_large_amount_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enbzb3/how_do_companies_that_deal_with_a_large_amount_of/', 'subreddit_subscribers': 203035, 'created_utc': 1723138194.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was thinking about how much data (logs, telemetry, etc.) gets generated by the systems that we use everyday to process the business data, and I started wondering what the best practices are for using/consuming this "data platform metadata"\n\nThe way I see it is you have a few diff categories: data quality checks after writes, system/cluster health and telemetry, billing, access logs. I feel like I\'m not leveraging all of this stuff as much as I should.\n\nWhat tools do you use most often, and for what? i.e. Splunk, DataDog, Grafana etc... or if you\'re one of the unlucky people who is tasked with optimizing for cost on a platform like Snowflake or DataBricks, do you use any monitoring page they offer as part of the product ?', 'author_fullname': 't2_89b8zvpm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you monitor data platforms you use in production?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enaz06', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723135788.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was thinking about how much data (logs, telemetry, etc.) gets generated by the systems that we use everyday to process the business data, and I started wondering what the best practices are for using/consuming this &quot;data platform metadata&quot;</p>\n\n<p>The way I see it is you have a few diff categories: data quality checks after writes, system/cluster health and telemetry, billing, access logs. I feel like I&#39;m not leveraging all of this stuff as much as I should.</p>\n\n<p>What tools do you use most often, and for what? i.e. Splunk, DataDog, Grafana etc... or if you&#39;re one of the unlucky people who is tasked with optimizing for cost on a platform like Snowflake or DataBricks, do you use any monitoring page they offer as part of the product ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1enaz06', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Typical_Priority3319'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enaz06/how_do_you_monitor_data_platforms_you_use_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enaz06/how_do_you_monitor_data_platforms_you_use_in/', 'subreddit_subscribers': 203035, 'created_utc': 1723135788.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "If I create a hybrid table on Apache Pinot from some mysql table. In which for realtime upsert table, I started a CDC connector which pushes data to Kafka, and pinot pulls data from it. For batch I pushed one time mysql dump to offline table. For realtime, upsert is working fine. It's returning a single record in case of duplicate id's. But if I am querying on hybrid table (OFFLINE + REALTIME), it's returning one record from Realtime and one from offline. If I am doing some aggreagtion also, it;s giving two result for same id, one from realtime and one from offline. I cannot create views on Pinot too. How to solve this ?", 'author_fullname': 't2_a0mp3x0h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Quries on Hybrid table on Apache Pinot', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emuoa6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723084302.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>If I create a hybrid table on Apache Pinot from some mysql table. In which for realtime upsert table, I started a CDC connector which pushes data to Kafka, and pinot pulls data from it. For batch I pushed one time mysql dump to offline table. For realtime, upsert is working fine. It&#39;s returning a single record in case of duplicate id&#39;s. But if I am querying on hybrid table (OFFLINE + REALTIME), it&#39;s returning one record from Realtime and one from offline. If I am doing some aggreagtion also, it;s giving two result for same id, one from realtime and one from offline. I cannot create views on Pinot too. How to solve this ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1emuoa6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='PlanktonRemarkable21'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emuoa6/quries_on_hybrid_table_on_apache_pinot/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emuoa6/quries_on_hybrid_table_on_apache_pinot/', 'subreddit_subscribers': 203035, 'created_utc': 1723084302.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have numerous Excel and PDF files on Google Drive containing T12 reports, all structured differently. The columns are named variably by different accountants. My task is to extract specific total values from these files, but the variables are located in various places and have different name variations, such as "Cost of reconstruction," "Cost of renovation," "Total renovation," etc.\n\nI initially built logic to look for the keyword "Total" and matched it with the relevant row. However, I face numerous errors because the program can\'t find "Total" in some files, where it is named differently, like "12 Trailing." I constantly receive new files with more name variations for the same values.\n\nIs it possible to use machine learning or other tools to help automate this process? What kind of logic and methods should I use to effectively solve this problem?', 'author_fullname': 't2_ezuzbprt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Extract values from Excel and PDF', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1encb2i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723138974.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have numerous Excel and PDF files on Google Drive containing T12 reports, all structured differently. The columns are named variably by different accountants. My task is to extract specific total values from these files, but the variables are located in various places and have different name variations, such as &quot;Cost of reconstruction,&quot; &quot;Cost of renovation,&quot; &quot;Total renovation,&quot; etc.</p>\n\n<p>I initially built logic to look for the keyword &quot;Total&quot; and matched it with the relevant row. However, I face numerous errors because the program can&#39;t find &quot;Total&quot; in some files, where it is named differently, like &quot;12 Trailing.&quot; I constantly receive new files with more name variations for the same values.</p>\n\n<p>Is it possible to use machine learning or other tools to help automate this process? What kind of logic and methods should I use to effectively solve this problem?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1encb2i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='avistafina'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1encb2i/extract_values_from_excel_and_pdf/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1encb2i/extract_values_from_excel_and_pdf/', 'subreddit_subscribers': 203035, 'created_utc': 1723138974.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi there,\n\nI have been exploring databricks and DE via a Udemy Course. I am venturing on my own to work on my own project with Mars Rover data.\n\nMy question(s) are:\n\nMy data has some a few nested structures in its schema.\n\ni.e. the data can be:\n\n\'photos.rover.name \'\n\nIf I go too deep and explode, I end up with duplicate and ambiguous column names because I also have:\n\n\'photos.camera.name\'\n\nI currently loaded it, exploded, and dropped "photos", so I have all the top level columns to query.  Is this too much for bronze stage?  \n\nShould it just have gone in to an S3 bucket directly from the API as a JSON for bronze? \n\nThen I extract from bronze and do all of the above for silver?\n\nThen I explode further and create and join tables properly for gold?  \nI feel like I\'m mixing stages.\n\nAlso, for these different stages, do I just create a separate location within the same S3 bucket? Or do I create separate buckets for each stage?\n\nThanks for the advice!\n\n', 'author_fullname': 't2_59q41s2x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Some data storage questions with Bronze->Silver', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enbf9w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723136854.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi there,</p>\n\n<p>I have been exploring databricks and DE via a Udemy Course. I am venturing on my own to work on my own project with Mars Rover data.</p>\n\n<p>My question(s) are:</p>\n\n<p>My data has some a few nested structures in its schema.</p>\n\n<p>i.e. the data can be:</p>\n\n<p>&#39;photos.rover.name &#39;</p>\n\n<p>If I go too deep and explode, I end up with duplicate and ambiguous column names because I also have:</p>\n\n<p>&#39;photos.camera.name&#39;</p>\n\n<p>I currently loaded it, exploded, and dropped &quot;photos&quot;, so I have all the top level columns to query.  Is this too much for bronze stage?  </p>\n\n<p>Should it just have gone in to an S3 bucket directly from the API as a JSON for bronze? </p>\n\n<p>Then I extract from bronze and do all of the above for silver?</p>\n\n<p>Then I explode further and create and join tables properly for gold?<br/>\nI feel like I&#39;m mixing stages.</p>\n\n<p>Also, for these different stages, do I just create a separate location within the same S3 bucket? Or do I create separate buckets for each stage?</p>\n\n<p>Thanks for the advice!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1enbf9w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Zeke_Grey'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enbf9w/some_data_storage_questions_with_bronzesilver/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enbf9w/some_data_storage_questions_with_bronzesilver/', 'subreddit_subscribers': 203035, 'created_utc': 1723136854.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.641+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "There is a Chill Data Summit event in San Carlos at Devil's Canyon Brewery on Aug 15 focused on teaching Apache Iceberg. Several committers and community experts will be attending. It's sponsored by Upsolver but there are no vendor talks, just tech talks from users and committers.\n\nHolden Karau, OSS engineer from Netflix will be leading the hands-on workshop.\n\nThis is a really good opportunity to meet folks in the DE community and learn more about Apache Iceberg.\n\nSign up [here](http://chilldatasummit.com/ontour/bayarea/Roy?utm_source=Reddit&utm_medium=Social&utm_campaign=Promo&utm_content=Register%20Now&utm_term=CDS%20Bay%20Area)\n\nUse code RDT50 for 50% off on the workshop.", 'author_fullname': 't2_vhiekvgo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Apache Iceberg event and workshop in Bay Area on Aug 15', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enamvd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723134973.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>There is a Chill Data Summit event in San Carlos at Devil&#39;s Canyon Brewery on Aug 15 focused on teaching Apache Iceberg. Several committers and community experts will be attending. It&#39;s sponsored by Upsolver but there are no vendor talks, just tech talks from users and committers.</p>\n\n<p>Holden Karau, OSS engineer from Netflix will be leading the hands-on workshop.</p>\n\n<p>This is a really good opportunity to meet folks in the DE community and learn more about Apache Iceberg.</p>\n\n<p>Sign up <a href="http://chilldatasummit.com/ontour/bayarea/Roy?utm_source=Reddit&amp;utm_medium=Social&amp;utm_campaign=Promo&amp;utm_content=Register%20Now&amp;utm_term=CDS%20Bay%20Area">here</a></p>\n\n<p>Use code RDT50 for 50% off on the workshop.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1enamvd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='royondata'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enamvd/apache_iceberg_event_and_workshop_in_bay_area_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enamvd/apache_iceberg_event_and_workshop_in_bay_area_on/', 'subreddit_subscribers': 203035, 'created_utc': 1723134973.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.642+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello Everyone,\n\nI am new to Airflow and want to create a DAG to read a file in s3 with source path and destination path columns and trigger a glue job concurrently while passing the above values  as parameters. There are ideally two steps to it.\n\nStep 1 - Read data from s3, clean and pass it into an XComm\n\nStep 2 - Create a function to iterate over the parameters and create glue job based on the data from comms,\n\nThe below code when inside a dag never triggers a Glue job. Can somebody help me with this\n\nwith TaskGroup('glue\\_jobs') as glue\\_jobs\\_group:\n\ndef create\\_glue\\_jobs(ti):\n\nparams\\_list = ti.xcom\\_pull(task\\_ids='get\\_params', key='return\\_value')\n\ntasks = \\[\\]\n\nfor i, params in enumerate(params\\_list):\n\nsrc\\_arg = params.get('src\\_arg')\n\ndest\\_arg = params.get('des\\_arg')\n\ntask\\_id = f'Glue\\_job\\_{i}'\n\nglue\\_job\\_task = create\\_glue\\_job\\_op(task\\_id, src\\_arg, dest\\_arg) --> Returns a Glue Job          Operator\n\ntasks.append(glue\\_job\\_task)\n\nglue\\_job\\_task\n\nget\\_params >> glue\\_jobs\\_group", 'author_fullname': 't2_vkm26ayq9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airflow help', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en9tkh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723133034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello Everyone,</p>\n\n<p>I am new to Airflow and want to create a DAG to read a file in s3 with source path and destination path columns and trigger a glue job concurrently while passing the above values  as parameters. There are ideally two steps to it.</p>\n\n<p>Step 1 - Read data from s3, clean and pass it into an XComm</p>\n\n<p>Step 2 - Create a function to iterate over the parameters and create glue job based on the data from comms,</p>\n\n<p>The below code when inside a dag never triggers a Glue job. Can somebody help me with this</p>\n\n<p>with TaskGroup(&#39;glue_jobs&#39;) as glue_jobs_group:</p>\n\n<p>def create_glue_jobs(ti):</p>\n\n<p>params_list = ti.xcom_pull(task_ids=&#39;get_params&#39;, key=&#39;return_value&#39;)</p>\n\n<p>tasks = []</p>\n\n<p>for i, params in enumerate(params_list):</p>\n\n<p>src_arg = params.get(&#39;src_arg&#39;)</p>\n\n<p>dest_arg = params.get(&#39;des_arg&#39;)</p>\n\n<p>task_id = f&#39;Glue_job_{i}&#39;</p>\n\n<p>glue_job_task = create_glue_job_op(task_id, src_arg, dest_arg) --&gt; Returns a Glue Job          Operator</p>\n\n<p>tasks.append(glue_job_task)</p>\n\n<p>glue_job_task</p>\n\n<p>get_params &gt;&gt; glue_jobs_group</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1en9tkh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Free-Way-5831'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en9tkh/airflow_help/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en9tkh/airflow_help/', 'subreddit_subscribers': 203035, 'created_utc': 1723133034.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.642+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_gxesw7ji', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bytebase 2.22.1 Released - Database Schema Change and Version Control for MySQL/PG/Oracle/MSSQL/Snowflake ...', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en1ur4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'https://b.thumbs.redditmedia.com/JWtfklpQGGkNI8Ci0Nsm5yeta94ICUcCX4k1uNf1Zac.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1723110564.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'bytebase.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.bytebase.com/changelog/bytebase-2-22-1/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?auto=webp&s=492f4a356d9d92db9da7e73d82e32457b659931b', 'width': 1600, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=108&crop=smart&auto=webp&s=69a645c323a4e23aeb4c2b7cc69bf07de05b0efb', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=216&crop=smart&auto=webp&s=30ce960b7a56deac4d6811a6dac15e370f29714d', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=320&crop=smart&auto=webp&s=83e2384e064a253b834d0ce71906ffb4f1462fc3', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=640&crop=smart&auto=webp&s=cfb3fba0ee2c72e439f4f2c8f53dfdeee24cb1b7', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=960&crop=smart&auto=webp&s=393d99c2b669160c422044a51e9b59cd033f8a44', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/-jf-arziRzNdF21Au8Cg9hKAco0ZvUyMc3WLlAh8hlE.jpg?width=1080&crop=smart&auto=webp&s=cd2f2a9d2d26b5ae84ce7d27d899d265af621e97', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'dHMsVVxB7dTRKe6OMdS8EJBdK2NwPpO_Ap2nyTqioWo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1en1ur4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Adela_freedom'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en1ur4/bytebase_2221_released_database_schema_change_and/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.bytebase.com/changelog/bytebase-2-22-1/', 'subreddit_subscribers': 203035, 'created_utc': 1723110564.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.642+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have access logs data of the users that keep on coming. Dailye we get near about 2 million access logs of the user. One user can access more than once also, so our problem statement is to keep the track of user access with entry\\_time(first access in a day) and exit\\_time(last access in a day). I have already prepared the flinkjob to do it which will calculate this information on runtime via streaming job.\n\nJust for the sale of understanding, this is data we will be calculating\n\nuser\\_name, location\\_name, entry\\_time, entry\\_door, exit\\_time, exit\\_door, etc.\n\nBy applying the aggregation on the current day data I can fetch the day wise user arrival information.\n\nBut the problem is I want to delete the past day data from this flink dynamic table since past day records are not requried. And as I mentined, since we daily get 2 million records, so if we won't delete the past day records then data will keep on adding to this flink table and with time, process will keep on getting slower since data is increasing at rapid rate.\n\nSo what to do to delete the past day data from the flink dynamic table since I only want to calculate the user arrival of the current day?\n\nFYI, I am getting this access logs data in the kafka, and from the kafka data I am applying the aggregation and then sending the aggregation data to another kafka, from there I am saving it to opensearch.\n\nI can share the code also if needed.\n\nDo let me know how to delete the past day data from the flink dynamic table\n\nI have tried with state TTL clear up, but it didn't help as I can see the past day data is still there.", 'author_fullname': 't2_tuerbtio', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Deletion of past data from the Flink Dynamic Table', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emy3xh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723095564.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have access logs data of the users that keep on coming. Dailye we get near about 2 million access logs of the user. One user can access more than once also, so our problem statement is to keep the track of user access with entry_time(first access in a day) and exit_time(last access in a day). I have already prepared the flinkjob to do it which will calculate this information on runtime via streaming job.</p>\n\n<p>Just for the sale of understanding, this is data we will be calculating</p>\n\n<p>user_name, location_name, entry_time, entry_door, exit_time, exit_door, etc.</p>\n\n<p>By applying the aggregation on the current day data I can fetch the day wise user arrival information.</p>\n\n<p>But the problem is I want to delete the past day data from this flink dynamic table since past day records are not requried. And as I mentined, since we daily get 2 million records, so if we won&#39;t delete the past day records then data will keep on adding to this flink table and with time, process will keep on getting slower since data is increasing at rapid rate.</p>\n\n<p>So what to do to delete the past day data from the flink dynamic table since I only want to calculate the user arrival of the current day?</p>\n\n<p>FYI, I am getting this access logs data in the kafka, and from the kafka data I am applying the aggregation and then sending the aggregation data to another kafka, from there I am saving it to opensearch.</p>\n\n<p>I can share the code also if needed.</p>\n\n<p>Do let me know how to delete the past day data from the flink dynamic table</p>\n\n<p>I have tried with state TTL clear up, but it didn&#39;t help as I can see the past day data is still there.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1emy3xh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ApprehensiveUse3133'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emy3xh/deletion_of_past_data_from_the_flink_dynamic_table/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emy3xh/deletion_of_past_data_from_the_flink_dynamic_table/', 'subreddit_subscribers': 203035, 'created_utc': 1723095564.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.642+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all! I am researching how organizations use embeddings and vector-based search technologies. Is anyone’s organization currently utilizing these technologies? If so, I’d love to connect to discuss this further as I have some questions. \n\nCheers \n', 'author_fullname': 't2_5dd5e4at', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Embeddings', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1enfpzs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723147187.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all! I am researching how organizations use embeddings and vector-based search technologies. Is anyone’s organization currently utilizing these technologies? If so, I’d love to connect to discuss this further as I have some questions. </p>\n\n<p>Cheers </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1enfpzs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='patatalyfe'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enfpzs/embeddings/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enfpzs/embeddings/', 'subreddit_subscribers': 203035, 'created_utc': 1723147187.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.643+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone, I'm curious about data engineering as a backend developer and computer science student. Are there any good resources to learn from scratch? Thanks a lot.", 'author_fullname': 't2_eb6b97ap', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for Resources to Start Learning Data Engineering from Scratch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1enfloj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723146894.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone, I&#39;m curious about data engineering as a backend developer and computer science student. Are there any good resources to learn from scratch? Thanks a lot.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1enfloj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Equivalent_Nerve_647'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enfloj/looking_for_resources_to_start_learning_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enfloj/looking_for_resources_to_start_learning_data/', 'subreddit_subscribers': 203035, 'created_utc': 1723146894.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.643+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone! I was wondering whether anyone has experience with (near-)real time OLAP use cases on GCP, especially when the data is coming in via Pub/Sub.\n\nThe idea is that it will power user facing dashboards (e.g. similar to Stripe Dashboard if you are familiar with it). Query response times should ideally be fast (<1sec) to give the user a great user experience. Newly arrived data should generally show up in the OLAP reports within minutes. \n\nI’ve been looking into Apache Pinot, Apache Druid and ClickHouse, but none of them then seem to integrate well with Pub/Sub. Read that some set up Kafka between Pub/Sub and one of these databases, just to get it to work, though this feels like overkill. \n\nWondering whether anyone worked on something similar before and what kind of tooling you went with.', 'author_fullname': 't2_13uo5xu3vv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Realtime OLAP on GCP', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1enezut', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723145417.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone! I was wondering whether anyone has experience with (near-)real time OLAP use cases on GCP, especially when the data is coming in via Pub/Sub.</p>\n\n<p>The idea is that it will power user facing dashboards (e.g. similar to Stripe Dashboard if you are familiar with it). Query response times should ideally be fast (&lt;1sec) to give the user a great user experience. Newly arrived data should generally show up in the OLAP reports within minutes. </p>\n\n<p>I’ve been looking into Apache Pinot, Apache Druid and ClickHouse, but none of them then seem to integrate well with Pub/Sub. Read that some set up Kafka between Pub/Sub and one of these databases, just to get it to work, though this feels like overkill. </p>\n\n<p>Wondering whether anyone worked on something similar before and what kind of tooling you went with.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1enezut', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Admirable_Hat_7871'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enezut/realtime_olap_on_gcp/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enezut/realtime_olap_on_gcp/', 'subreddit_subscribers': 203035, 'created_utc': 1723145417.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.643+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Is there a library that offers a DSL like experience to define data transformations similar to Kafka connect SMT ?', 'author_fullname': 't2_hmsc4tzi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Kafka connect SMT as a library ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1enejxb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723144321.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Is there a library that offers a DSL like experience to define data transformations similar to Kafka connect SMT ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1enejxb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No_Direction_5276'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enejxb/kafka_connect_smt_as_a_library/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enejxb/kafka_connect_smt_as_a_library/', 'subreddit_subscribers': 203035, 'created_utc': 1723144321.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.643+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I would like to hear about your experiences regarding the transformation that occurs after ingesting raw data. What do you consider to be the most "correct" approach? What scenarios have you encountered problems with? What approaches do you prefer?\n\nShould you apply only what is necessary to process the data, such as filtering rows, handling nulls, etc.? Should you integrate tables to create a single source for consumption, or keep them separate by source?\n\nRegarding history:\n\n* Should you maintain history? If so, with or without SCD2 (Slowly Changing Dimension Type 2)?\n* Should you keep only the most current row (for dimensions and transactional data)? If so, if necessary, how should you reconstruct a table in the next layer without history?\n\nhttps://preview.redd.it/mtudsvro0hhd1.png?width=1523&format=png&auto=webp&s=c965c9df905ad09098d5fab92d313e846c67ef3b\n\n', 'author_fullname': 't2_2t4hdsut', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Lakehouse layer approach', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'mtudsvro0hhd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=108&crop=smart&auto=webp&s=8f5814dc7a584924acee3c426c7da23a179435a1'}, {'y': 107, 'x': 216, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=216&crop=smart&auto=webp&s=f5167e0308ccd6cb1c7474d05569fd169a1903ff'}, {'y': 158, 'x': 320, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=320&crop=smart&auto=webp&s=ba589fe3dacef0b6bacc2ccc4ff8ff7188c88700'}, {'y': 317, 'x': 640, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=640&crop=smart&auto=webp&s=db038eacc15a6f1000319f75dd5e4630a3b3cd56'}, {'y': 476, 'x': 960, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=960&crop=smart&auto=webp&s=27f40a3b4d76608ba8a501ce920578a7648de2eb'}, {'y': 536, 'x': 1080, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=1080&crop=smart&auto=webp&s=c1260f945a71b4f2c2487c2da49800b05c930fe8'}], 's': {'y': 756, 'x': 1523, 'u': 'https://preview.redd.it/mtudsvro0hhd1.png?width=1523&format=png&auto=webp&s=c965c9df905ad09098d5fab92d313e846c67ef3b'}, 'id': 'mtudsvro0hhd1'}}, 'name': 't3_1enbhhh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/bJDR3jFSxM6sY8aeUbxvVXmi2dpbqH8Ku1deQcQiSH8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723137010.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I would like to hear about your experiences regarding the transformation that occurs after ingesting raw data. What do you consider to be the most &quot;correct&quot; approach? What scenarios have you encountered problems with? What approaches do you prefer?</p>\n\n<p>Should you apply only what is necessary to process the data, such as filtering rows, handling nulls, etc.? Should you integrate tables to create a single source for consumption, or keep them separate by source?</p>\n\n<p>Regarding history:</p>\n\n<ul>\n<li>Should you maintain history? If so, with or without SCD2 (Slowly Changing Dimension Type 2)?</li>\n<li>Should you keep only the most current row (for dimensions and transactional data)? If so, if necessary, how should you reconstruct a table in the next layer without history?</li>\n</ul>\n\n<p><a href="https://preview.redd.it/mtudsvro0hhd1.png?width=1523&amp;format=png&amp;auto=webp&amp;s=c965c9df905ad09098d5fab92d313e846c67ef3b">https://preview.redd.it/mtudsvro0hhd1.png?width=1523&amp;format=png&amp;auto=webp&amp;s=c965c9df905ad09098d5fab92d313e846c67ef3b</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1enbhhh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ltofanelli'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enbhhh/lakehouse_layer_approach/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enbhhh/lakehouse_layer_approach/', 'subreddit_subscribers': 203035, 'created_utc': 1723137010.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.643+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm playing with a proof-of-concept on-prem Lakehouse with off the shelf open-source tools. So far I've integrated Minio + Trino + Hive + Iceberg and it went smoothly. However I'm concerned that I was not able to find any resources about rebuilding catalog layer from metadatas residing on S3/Minio.\n\nThis is a scenario for when Hive Metastore fails, or I want it swapped for anything else. Isn't Iceberg catalog supposed to be self-sufficient and auto-healing solely from storage layer?\n\nWhat am I missing guys?\n\n[https://iceberg.apache.org/spec/](https://iceberg.apache.org/spec/)\n\nhttps://preview.redd.it/ogknw8g9nghd1.png?width=1248&format=png&auto=webp&s=4545d7eac9475e54d09c4dc2b3c3abbd63055741\n\n\n\n  \n", 'author_fullname': 't2_hn1p2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to rebuild Iceberg catalog from metadata layer on S3', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ogknw8g9nghd1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 111, 'x': 108, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=108&crop=smart&auto=webp&s=4259a540720910630ee6d25bfc79de0bff4871c9'}, {'y': 223, 'x': 216, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=216&crop=smart&auto=webp&s=ef9c1dcbc6c888804c0dabb54d55feb20dee7f1a'}, {'y': 330, 'x': 320, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=320&crop=smart&auto=webp&s=7594157cc2d9661c764ec9ab85f7d464a96f6435'}, {'y': 661, 'x': 640, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=640&crop=smart&auto=webp&s=4b649e933de3ef9d2959f4ec674d287b4df10b59'}, {'y': 992, 'x': 960, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=960&crop=smart&auto=webp&s=2a1bc6a72a495ba5a0f2ecfca08fabac186d2893'}, {'y': 1116, 'x': 1080, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=1080&crop=smart&auto=webp&s=428286a33665baa17f78989f615d5dbe6eb4ed01'}], 's': {'y': 1290, 'x': 1248, 'u': 'https://preview.redd.it/ogknw8g9nghd1.png?width=1248&format=png&auto=webp&s=4545d7eac9475e54d09c4dc2b3c3abbd63055741'}, 'id': 'ogknw8g9nghd1'}}, 'name': 't3_1en97dt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/qgk7WFZwKU6XnvnIhIjrUOUL0Vet8UdGILAV9uGrdqM.jpg', 'edited': 1723131996.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723131559.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m playing with a proof-of-concept on-prem Lakehouse with off the shelf open-source tools. So far I&#39;ve integrated Minio + Trino + Hive + Iceberg and it went smoothly. However I&#39;m concerned that I was not able to find any resources about rebuilding catalog layer from metadatas residing on S3/Minio.</p>\n\n<p>This is a scenario for when Hive Metastore fails, or I want it swapped for anything else. Isn&#39;t Iceberg catalog supposed to be self-sufficient and auto-healing solely from storage layer?</p>\n\n<p>What am I missing guys?</p>\n\n<p><a href="https://iceberg.apache.org/spec/">https://iceberg.apache.org/spec/</a></p>\n\n<p><a href="https://preview.redd.it/ogknw8g9nghd1.png?width=1248&amp;format=png&amp;auto=webp&amp;s=4545d7eac9475e54d09c4dc2b3c3abbd63055741">https://preview.redd.it/ogknw8g9nghd1.png?width=1248&amp;format=png&amp;auto=webp&amp;s=4545d7eac9475e54d09c4dc2b3c3abbd63055741</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1en97dt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='georgegach'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en97dt/how_to_rebuild_iceberg_catalog_from_metadata/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en97dt/how_to_rebuild_iceberg_catalog_from_metadata/', 'subreddit_subscribers': 203035, 'created_utc': 1723131559.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In azure synapse I\'m calling REST against a 3rd party tool. Our team adds properties to this tool all the time, and I\'m experimenting with making the query dynamic so that I don\'t have to keep updating the pipeline.\n\nFirst, I make a rest api call to and endpoint that gives me a table of all the properties I can use in another endpoint - lets call this "properties". I stick the query result in a staging table in the SQL pool.\n\nI then use a stored procedure to string agg all the values from one column (property name) in that table into a big ass concatenated string and store that in a table that is one column one row. Lets call that AllPropertiesString.\n\nIn a second pipeline, I use a lookup to pull AllPropertiesString and stick it on the relativeURL as a suffix.\n\n\n\nI feel like there has to be a more elegant way of doing this. My method feels caveman-ish.\n\nAny ideas?', 'author_fullname': 't2_8xxrr2vi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure Synapse - using output from one REST call as property list for another?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en4oth', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723120174.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In azure synapse I&#39;m calling REST against a 3rd party tool. Our team adds properties to this tool all the time, and I&#39;m experimenting with making the query dynamic so that I don&#39;t have to keep updating the pipeline.</p>\n\n<p>First, I make a rest api call to and endpoint that gives me a table of all the properties I can use in another endpoint - lets call this &quot;properties&quot;. I stick the query result in a staging table in the SQL pool.</p>\n\n<p>I then use a stored procedure to string agg all the values from one column (property name) in that table into a big ass concatenated string and store that in a table that is one column one row. Lets call that AllPropertiesString.</p>\n\n<p>In a second pipeline, I use a lookup to pull AllPropertiesString and stick it on the relativeURL as a suffix.</p>\n\n<p>I feel like there has to be a more elegant way of doing this. My method feels caveman-ish.</p>\n\n<p>Any ideas?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1en4oth', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Apprehensive-Box281'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en4oth/azure_synapse_using_output_from_one_rest_call_as/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en4oth/azure_synapse_using_output_from_one_rest_call_as/', 'subreddit_subscribers': 203035, 'created_utc': 1723120174.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Did anyone use UCX for UC migration and what’s their take on it?\n', 'author_fullname': 't2_abg15fr62', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'UCX Databricks ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en2hzf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723112979.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Did anyone use UCX for UC migration and what’s their take on it?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1en2hzf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Glum_Attorney_6755'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en2hzf/ucx_databricks/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1en2hzf/ucx_databricks/', 'subreddit_subscribers': 203035, 'created_utc': 1723112979.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_149tyuiyao', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bridging Backend and Data Engineering: Communicating Through Events', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 91, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1en2cpq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Zibo9hlUpNixuThTL5wxHFYi96vj03FHi7x_KWnfwPE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1723112403.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/@pliutau/bridging-backend-and-data-engineering-communicating-through-events-f699d6200bb1', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?auto=webp&s=13b2a74c9e678850b87f3dce1b195fa79f1f58e7', 'width': 1200, 'height': 781}, 'resolutions': [{'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=108&crop=smart&auto=webp&s=f0075cdf337b7f9549ef57025948c0ce2f4bd4f9', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=216&crop=smart&auto=webp&s=27fff7c0ba34154fbb375a85423359d036488baf', 'width': 216, 'height': 140}, {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=320&crop=smart&auto=webp&s=e516b8b8e79100926a5eca1c288c628cad954767', 'width': 320, 'height': 208}, {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=640&crop=smart&auto=webp&s=66edbc5a5a71d0600c340e28cb21de2fcb82c89b', 'width': 640, 'height': 416}, {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=960&crop=smart&auto=webp&s=3a5f33aead0520f166eff227a3b226665fc95935', 'width': 960, 'height': 624}, {'url': 'https://external-preview.redd.it/R8_RPEyTCE7jOWBMFnGTeNpQ0YP_xjTCdksS1O4xVzw.jpg?width=1080&crop=smart&auto=webp&s=91bb84f52d78a1fca70ea8da51794beca486705c', 'width': 1080, 'height': 702}], 'variants': {}, 'id': 'RX-YvHGIUkan2R8utpKs_bF6iomXve7ML3vaibzy9wI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1en2cpq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='der_gopher'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1en2cpq/bridging_backend_and_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/@pliutau/bridging-backend-and-data-engineering-communicating-through-events-f699d6200bb1', 'subreddit_subscribers': 203035, 'created_utc': 1723112403.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm trying to automate my a report using R and SQL.\nI've made the necessary connections to my DB, but on trying to execute the SQL query I get no results in the R object.\nThis is the part I speak of :\nResults <- dbGetQuery(con_160,query)\nI can't share screen shots as this is a work task.\nAll the necessary libraries have been installed and the SQL query works perfectly when run in SQL server.\nThe connection is valid as I can still query driectly from R and get results. \nPlease share any ideas on what the problem could be.\n", 'author_fullname': 't2_exf3ik6x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help me debug my code', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1enh031', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723150326.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m trying to automate my a report using R and SQL.\nI&#39;ve made the necessary connections to my DB, but on trying to execute the SQL query I get no results in the R object.\nThis is the part I speak of :\nResults &lt;- dbGetQuery(con_160,query)\nI can&#39;t share screen shots as this is a work task.\nAll the necessary libraries have been installed and the SQL query works perfectly when run in SQL server.\nThe connection is valid as I can still query driectly from R and get results. \nPlease share any ideas on what the problem could be.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1enh031', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AppropriateSeason309'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1enh031/help_me_debug_my_code/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1enh031/help_me_debug_my_code/', 'subreddit_subscribers': 203035, 'created_utc': 1723150326.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey everyone,\nI hope this is the right place to ask this question. \nI have a tabular model currently deployed in PowerBi premium using the import mode design. The problem is that I have a large model +75GB so I'm always struggling with this and plus, capacities are too expensive to just keep on scaling it up. I was thinking of using directquery but I cannot set partitions, and almost all data is on one specific table, so whenever a user does something on the frontend the huge table is queried (select * from hugetable). How could I overcome this?\nSorry if I'm not explaining myself well.\nThanks", 'author_fullname': 't2_w8u4qnfy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tabular model design', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1eng4a0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723148138.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,\nI hope this is the right place to ask this question. \nI have a tabular model currently deployed in PowerBi premium using the import mode design. The problem is that I have a large model +75GB so I&#39;m always struggling with this and plus, capacities are too expensive to just keep on scaling it up. I was thinking of using directquery but I cannot set partitions, and almost all data is on one specific table, so whenever a user does something on the frontend the huge table is queried (select * from hugetable). How could I overcome this?\nSorry if I&#39;m not explaining myself well.\nThanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eng4a0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LankyOpportunity8363'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eng4a0/tabular_model_design/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eng4a0/tabular_model_design/', 'subreddit_subscribers': 203035, 'created_utc': 1723148138.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.644+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff888d0ed0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have 2 years work experience as a data analyst working for a small consulting company working mostly for pharmaceutical companies. I have built dash boards and use python quite regular to perform data analysis. ', 'author_fullname': 't2_2z2wymyd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Can I become a DE with a business and data analytics BA?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1emy3wj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.44, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723095561.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have 2 years work experience as a data analyst working for a small consulting company working mostly for pharmaceutical companies. I have built dash boards and use python quite regular to perform data analysis. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1emy3wj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cailloudragonballs'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1emy3wj/can_i_become_a_de_with_a_business_and_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1emy3wj/can_i_become_a_de_with_a_business_and_data/', 'subreddit_subscribers': 203035, 'created_utc': 1723095561.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-08T21:16:20.645+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-08-08T21:16:20.646+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-08T21:16:20.668+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-08-08T21:16:17.793694+00:00, execution_date=20240808T211617, start_date=20240808T211619, end_date=20240808T211620
[2024-08-08T21:16:20.710+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2024-08-08T21:16:20.724+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-08-08T21:16:20.724+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
